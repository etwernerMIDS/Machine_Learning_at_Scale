# Assignments

## Erin Werner

### Table of Contents

* [Homework 1](https://github.com/etwernerMIDS/Machine_Learning_at_Scale/tree/main/Assignments/HW1)

The main focus of Project 1 is querying the data. This is accomplished with SQL while utilizing Google Cloud Platform (GCP) and BiqQuery. The queries answer business-driven questions using public datasets housed in GCP. The datasets are accessed through the web UI (BiqQuery) and command-line tools. Analysis and insights are performed in Jupyter Notebooks.

* [Homework 2](https://github.com/etwernerMIDS/Machine_Learning_at_Scale/tree/main/Assignments/HW2)

The main focus of Project 2 is to take raw event data and make it queryable. This is accomplished by publishing and consuming the .json messages with Kafka, unrolling and transforming the messages with Spark, and then landing the messages in Hadoop (HDFS) for long term storage. Once the events are in HDFS, the data is ready to be queried. 

* [Homework 3](https://github.com/etwernerMIDS/Machine_Learning_at_Scale/tree/main/Assignments/HW3)

The main focus of Project 3 is creating the entire data engineering pipeline, from end-to-end. A stream of data events are generated using Apache Bench, those events are then consumed by Kafka, filtered by Spark, and then stored in HDFS/Parquet. The data is then available for analysis using Presto.

* [Homework 4](https://github.com/etwernerMIDS/Machine_Learning_at_Scale/tree/main/Assignments/HW4)

The main focus of Project 2 is to take raw event data and make it queryable. This is accomplished by publishing and consuming the .json messages with Kafka, unrolling and transforming the messages with Spark, and then landing the messages in Hadoop (HDFS) for long term storage. Once the events are in HDFS, the data is ready to be queried. 

* [Homework 5](https://github.com/etwernerMIDS/Machine_Learning_at_Scale/tree/main/Assignments/HW5)

The main focus of Project 3 is creating the entire data engineering pipeline, from end-to-end. A stream of data events are generated using Apache Bench, those events are then consumed by Kafka, filtered by Spark, and then stored in HDFS/Parquet. The data is then available for analysis using Presto.

*All of these projects were completed for UCB's Master of Data Science Course W261 - Machine Learning at Scale. These were completed using a Google Cloud Platform Python Notebook Instance.* 
